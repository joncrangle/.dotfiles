[{"id":"mixture_of_agents","user_id":"b37295bf-e323-4f98-b405-4b65f91f4731","name":"Mixture of Agents","type":"action","content":"\"\"\"\"\ntitle: Mixture of Agents Action\nauthor: MaxKerkula\nversion: 0.4\nrequired_open_webui_version: 0.3.9\n\"\"\"\n\nfrom pydantic import BaseModel, Field\nfrom typing import Optional, List, Callable, Awaitable\nimport aiohttp\nimport random\nimport asyncio\nimport time\n\n\nclass Action:\n    class Valves(BaseModel):\n        models: List[str] = Field(\n            default=[], description=\"List of models to use in the MoA architecture.\"\n        )\n        aggregator_model: str = Field(\n            default=\"\", description=\"Model to use for aggregation tasks.\"\n        )\n        openai_api_base: str = Field(\n            default=\"http://host.docker.internal:11434/v1\",\n            description=\"Base URL for Ollama API.\",\n        )\n        num_layers: int = Field(default=1, description=\"Number of MoA layers.\")\n        num_agents_per_layer: int = Field(\n            default=3, description=\"Number of agents to use in each layer.\"\n        )\n        emit_interval: float = Field(\n            default=1.0, description=\"Interval in seconds between status emissions\"\n        )\n        enable_status_indicator: bool = Field(\n            default=True, description=\"Enable or disable status indicator emissions\"\n        )\n\n    def __init__(self):\n        self.valves = self.Valves()\n        self.last_emit_time = 0\n\n    async def action(\n        self,\n        body: dict,\n        __user__: Optional[dict] = None,\n        __event_emitter__: Callable[[dict], Awaitable[None]] = None,\n        __event_call__: Callable[[dict], Awaitable[dict]] = None,\n    ) -> Optional[dict]:\n        await self.emit_status(\n            __event_emitter__, \"info\", \"Starting Mixture of Agents process\", False\n        )\n\n        try:\n            await self.validate_models(__event_emitter__)\n        except ValueError as e:\n            await self.emit_status(__event_emitter__, \"error\", str(e), True)\n            return {\"error\": str(e)}\n\n        messages = body.get(\"messages\", [])\n        if not messages:\n            error_msg = \"No messages found in the request body\"\n            await self.emit_status(__event_emitter__, \"error\", error_msg, True)\n            return {\"error\": error_msg}\n\n        last_message = messages[-1][\"content\"]\n        moa_response = await self.moa_process(last_message, __event_emitter__)\n\n        if moa_response.startswith(\"Error:\"):\n            await self.emit_status(__event_emitter__, \"error\", moa_response, True)\n            return {\"error\": moa_response}\n\n        body[\"messages\"].append({\"role\": \"assistant\", \"content\": moa_response})\n        await self.emit_status(\n            __event_emitter__, \"info\", \"Mixture of Agents process completed\", True\n        )\n        return body\n\n    async def validate_models(\n        self, __event_emitter__: Callable[[dict], Awaitable[None]] = None\n    ):\n        await self.emit_status(__event_emitter__, \"info\", \"Validating models\", False)\n        valid_models = []\n        for model in self.valves.models:\n            response = await self.query_ollama(model, \"Test prompt\", __event_emitter__)\n            if not response.startswith(\"Error:\"):\n                valid_models.append(model)\n\n        if not valid_models:\n            error_msg = (\n                \"No valid models available. Please check your model configurations.\"\n            )\n            await self.emit_status(__event_emitter__, \"error\", error_msg, True)\n            raise ValueError(error_msg)\n\n        self.valves.models = valid_models\n        await self.emit_status(\n            __event_emitter__, \"info\", f\"Validated {len(valid_models)} models\", False\n        )\n\n    async def moa_process(\n        self, prompt: str, __event_emitter__: Callable[[dict], Awaitable[None]] = None\n    ) -> str:\n        if (\n            not self.valves.models\n            or not self.valves.aggregator_model\n            or not self.valves.openai_api_base\n        ):\n            error_msg = \"Configuration error: Models, aggregator model, or API base URL not set.\"\n            await self.emit_status(__event_emitter__, \"error\", error_msg, True)\n            return f\"Error: {error_msg}\"\n\n        if len(self.valves.models) < self.valves.num_agents_per_layer:\n            error_msg = f\"Not enough models available. Required: {self.valves.num_agents_per_layer}, Available: {len(self.valves.models)}\"\n            await self.emit_status(__event_emitter__, \"error\", error_msg, True)\n            return f\"Error: {error_msg}\"\n\n        layer_outputs = []\n        for layer in range(self.valves.num_layers):\n            await self.emit_status(\n                __event_emitter__,\n                \"info\",\n                f\"Processing layer {layer + 1}/{self.valves.num_layers}\",\n                False,\n            )\n\n            layer_agents = random.sample(\n                self.valves.models,\n                self.valves.num_agents_per_layer,\n            )\n\n            tasks = [\n                self.process_agent(\n                    prompt, agent, layer, i, layer_outputs, __event_emitter__\n                )\n                for i, agent in enumerate(layer_agents)\n            ]\n            current_layer_outputs = await asyncio.gather(*tasks)\n\n            valid_outputs = [\n                output\n                for output in current_layer_outputs\n                if not output.startswith(\"Error:\")\n            ]\n            if not valid_outputs:\n                error_msg = (\n                    f\"No valid responses received from any agent in layer {layer + 1}\"\n                )\n                await self.emit_status(__event_emitter__, \"error\", error_msg, True)\n                return f\"Error: {error_msg}\"\n\n            layer_outputs.append(valid_outputs)\n            await self.emit_status(\n                __event_emitter__,\n                \"info\",\n                f\"Completed layer {layer + 1}/{self.valves.num_layers}\",\n                False,\n            )\n\n        await self.emit_status(\n            __event_emitter__, \"info\", \"Creating final aggregator prompt\", False\n        )\n        final_prompt = self.create_final_aggregator_prompt(prompt, layer_outputs)\n\n        await self.emit_status(\n            __event_emitter__, \"info\", \"Generating final response\", False\n        )\n        final_response = await self.query_ollama(\n            self.valves.aggregator_model, final_prompt, __event_emitter__\n        )\n\n        if final_response.startswith(\"Error:\"):\n            await self.emit_status(\n                __event_emitter__, \"error\", \"Failed to generate final response\", True\n            )\n            return f\"Error: Failed to generate final response. Last error: {final_response}\"\n\n        return final_response\n\n    async def process_agent(\n        self, prompt, agent, layer, agent_index, layer_outputs, __event_emitter__\n    ):\n        await self.emit_status(\n            __event_emitter__,\n            \"info\",\n            f\"Querying agent {agent_index + 1} in layer {layer + 1}\",\n            False,\n        )\n\n        if layer == 0:\n            response = await self.query_ollama(agent, prompt, __event_emitter__)\n        else:\n            await self.emit_status(\n                __event_emitter__,\n                \"info\",\n                f\"Creating aggregator prompt for layer {layer + 1}\",\n                False,\n            )\n            aggregator_prompt = self.create_aggregator_prompt(prompt, layer_outputs[-1])\n            response = await self.query_ollama(\n                self.valves.aggregator_model, aggregator_prompt, __event_emitter__\n            )\n\n        await self.emit_status(\n            __event_emitter__,\n            \"info\",\n            f\"Received response from agent {agent_index + 1} in layer {layer + 1}\",\n            False,\n        )\n        return response\n\n    def create_aggregator_prompt(\n        self, original_prompt: str, previous_responses: List[str]\n    ) -> str:\n        aggregator_prompt = (\n            f\"Original prompt: {original_prompt}\\n\\nPrevious responses:\\n\"\n        )\n        for i, response in enumerate(previous_responses, 1):\n            aggregator_prompt += f\"{i}. {response}\\n\\n\"\n        aggregator_prompt += \"Based on the above responses and the original prompt, provide an improved and comprehensive answer:\"\n        return aggregator_prompt\n\n    def create_final_aggregator_prompt(\n        self, original_prompt: str, all_layer_outputs: List[List[str]]\n    ) -> str:\n        final_prompt = (\n            f\"Original prompt: {original_prompt}\\n\\nResponses from all layers:\\n\"\n        )\n        for layer, responses in enumerate(all_layer_outputs, 1):\n            final_prompt += f\"Layer {layer}:\\n\"\n            for i, response in enumerate(responses, 1):\n                final_prompt += f\" {i}. {response}\\n\\n\"\n        final_prompt += (\n            \"Considering all the responses from different layers and the original prompt, provide a final, comprehensive answer that strictly adheres to the original request:\\n\"\n            \"1. Incorporate relevant information from all previous responses seamlessly.\\n\"\n            \"2. Avoid referencing or acknowledging previous responses explicitly unless directed by the prompt.\\n\"\n            \"3. Provide a complete and detailed reply addressing the original prompt.\"\n        )\n        return final_prompt\n\n    async def query_ollama(\n        self,\n        model: str,\n        prompt: str,\n        __event_emitter__: Callable[[dict], Awaitable[None]] = None,\n    ) -> str:\n        url = f\"{self.valves.openai_api_base}/chat/completions\"\n        headers = {\"Content-Type\": \"application/json\"}\n        data = {\"model\": model, \"messages\": [{\"role\": \"user\", \"content\": prompt}]}\n\n        try:\n            await self.emit_status(\n                __event_emitter__,\n                \"info\",\n                f\"Sending API request to model: {model}\",\n                False,\n            )\n\n            async with aiohttp.ClientSession() as session:\n                async with session.post(url, headers=headers, json=data) as response:\n                    if response.status == 404:\n                        error_message = f\"Model '{model}' not found. Please check if the model is available and correctly specified.\"\n                        await self.emit_status(\n                            __event_emitter__, \"error\", error_message, True\n                        )\n                        return f\"Error: {error_message}\"\n\n                    response.raise_for_status()\n                    result = await response.json()\n\n            await self.emit_status(\n                __event_emitter__,\n                \"info\",\n                f\"Received API response from model: {model}\",\n                False,\n            )\n\n            return result[\"choices\"][0][\"message\"][\"content\"]\n        except aiohttp.ClientResponseError as e:\n            error_message = f\"HTTP error querying Ollama API for model {model}: {e.status}, {e.message}\"\n            await self.emit_status(__event_emitter__, \"error\", error_message, True)\n            print(error_message)\n            return f\"Error: Unable to query model {model} due to HTTP error {e.status}\"\n        except aiohttp.ClientError as e:\n            error_message = (\n                f\"Network error querying Ollama API for model {model}: {str(e)}\"\n            )\n            await self.emit_status(__event_emitter__, \"error\", error_message, True)\n            print(error_message)\n            return f\"Error: Unable to query model {model} due to network error\"\n        except Exception as e:\n            error_message = (\n                f\"Unexpected error querying Ollama API for model {model}: {str(e)}\"\n            )\n            await self.emit_status(__event_emitter__, \"error\", error_message, True)\n            print(error_message)\n            return f\"Error: Unable to query model {model} due to unexpected error\"\n\n    async def emit_status(\n        self,\n        __event_emitter__: Callable[[dict], Awaitable[None]],\n        level: str,\n        message: str,\n        done: bool,\n    ):\n        current_time = time.time()\n        if (\n            __event_emitter__\n            and self.valves.enable_status_indicator\n            and (\n                current_time - self.last_emit_time >= self.valves.emit_interval or done\n            )\n        ):\n            await __event_emitter__(\n                {\n                    \"type\": \"status\",\n                    \"data\": {\n                        \"status\": \"complete\" if done else \"in_progress\",\n                        \"level\": level,\n                        \"description\": message,\n                        \"done\": done,\n                    },\n                }\n            )\n            self.last_emit_time = current_time\n\n    async def on_start(self):\n        print(\"Mixture of Agents Action started\")\n\n    async def on_stop(self):\n        print(\"Mixture of Agents Action stopped\")\n\n\n# The implementation approach and improvements are based on best practices and examples from GitHub repositories such as:\n# - [Together MoA Implementation](https://github.com/togethercomputer/MoA)\n# - [MX-Goliath/MoA-Ollama](https://github.com/MX-Goliath/MoA-Ollama)\n# - [AI-MickyJ/Mixture-of-Agents](https://github.com/AI-MickyJ/Mixture-of-Agents)\n","meta":{"description":"Button that allows for the collective strengths of multiple models to be leveraged in a layered, iterative process, potentially leading to higher quality responses.","manifest":{}},"is_active":true,"is_global":true,"updated_at":1728685229,"created_at":1728685136}]
